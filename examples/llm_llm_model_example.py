# examples/model_LLMmodel_example.py

import sys
import os

# Adjust the path to include the src directory
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# Try to import the module
try:
    from src.model.LLMmodel import LLMmodel
    # from src.model.LLMmodel import LLMConfig, AnotherHelperClass
except ImportError:
    print("Warning: Could not import LLMmodel from src.model.LLMmodel.")
    print("Using a dummy LLMmodel class for demonstration purposes.")

    class LLMmodel:
        """
        Dummy LLMmodel class for demonstration if the actual module/class
        cannot be imported or its structure is unknown.
        """
        def __init__(self, model_name="default_llm", api_key=None, config=None):
            self.model_name = model_name
            self.api_key = api_key # Should not be printed directly for security
            self.config = config if config else {}
            print(f"Dummy LLMmodel initialized for model: {self.model_name}. Config: {self.config}")
            if api_key:
                print("API key provided (masked for security in real logs).")

        def generate_text(self, prompt, max_tokens=150, temperature=0.7, stop_sequences=None):
            """
            Dummy method to simulate text generation.
            """
            print(f"\nGenerating text for prompt: '{prompt[:50]}...'")
            print(f"Parameters: max_tokens={max_tokens}, temperature={temperature}, stop_sequences={stop_sequences}")

            # Simulate LLM response
            response_text = f"This is a simulated response from '{self.model_name}' for the prompt. "
            response_text += f"It would normally be generated by the actual LLM based on '{prompt}'. Example: The capital of France is Paris."
            if len(response_text) > max_tokens :
                 response_text = response_text[:max_tokens]

            print(f"Generated Text (dummy): '{response_text}'")
            return response_text

        def generate_sql_from_natural_language(self, natural_language_query, db_schema, examples=None):
            """
            Dummy method specific to SQL generation.
            """
            print(f"\nGenerating SQL from NL: '{natural_language_query}'")
            print(f"DB Schema context: {db_schema}")
            if examples:
                print(f"Few-shot examples provided: {len(examples)}")

            # Simulate SQL generation
            sql_query = f"SELECT column1, column2 FROM {db_schema.get('tables', ['default_table'])[0].get('name', 'some_table')} WHERE condition_based_on='{natural_language_query.replace(' ', '_')}';"
            print(f"Generated SQL (dummy): {sql_query}")
            return sql_query

        def get_text_embedding(self, text_input):
            """
            Dummy method to simulate getting text embeddings.
            """
            print(f"\nGetting embedding for text: '{text_input[:50]}...'")
            # Simulate an embedding vector
            embedding = [0.1 * len(text_input) % 1, 0.2, -0.05, 0.33 * len(text_input) % 1]
            print(f"Generated Embedding (dummy): {embedding[:5]}... (dim: {len(embedding)})")
            return embedding

        def get_model_capabilities(self):
            """
            Dummy method to return capabilities of the loaded LLM.
            """
            capabilities = {
                "model_name": self.model_name,
                "type": "dummy_generative_text_and_embedding_model",
                "supports_sql_generation": True,
                "max_context_length": 4096 if "gpt-4" in self.model_name else 2048
            }
            print(f"\nModel Capabilities (dummy): {capabilities}")
            return capabilities

def main():
    print("--- LLMmodel Module Example ---")

    # Instantiate LLMmodel, typically with a model name and API key/config
    try:
        # Configuration for the LLM
        llm_config = {
            "temperature": 0.5,
            "timeout": 120,
            "retry_attempts": 3,
            # Other specific parameters for the LLM API
        }
        # In a real scenario, API key would be securely managed (e.g., env variables)
        llm_instance = LLMmodel(model_name="gpt-3.5-turbo", api_key="YOUR_LLM_API_KEY_HERE", config=llm_config)
    except NameError: # Fallback for dummy
         llm_instance = LLMmodel(model_name="dummy-llm-v1", api_key="DUMMY_API_KEY", config={"temperature": 0.5})


    # Example 1: Generate general text
    print("\n[Example 1: Generate Text]")
    prompt1 = "Explain the concept of a relational database in simple terms."
    generated_text = llm_instance.generate_text(prompt=prompt1, max_tokens=100, temperature=0.6)
    print(f"Prompt: {prompt1}\nLLM Response: {generated_text}")

    # Example 2: Generate SQL from a natural language query
    # This assumes the LLMmodel might have a specialized method for this.
    if hasattr(llm_instance, "generate_sql_from_natural_language"):
        print("\n[Example 2: Generate SQL from Natural Language]")
        nl_query = "Show me all employees in the 'Sales' department hired after 2022."
        schema = {
            "tables": [
                {"name": "employees", "columns": ["employee_id", "name", "department", "hire_date"]},
                {"name": "departments", "columns": ["department_id", "name"]}
            ],
            "foreign_keys": []
        }
        # Optional: provide few-shot examples
        few_shot_examples = [
            {"nl": "List names of all departments", "sql": "SELECT name FROM departments;"},
            {"nl": "Find employees named John", "sql": "SELECT * FROM employees WHERE name = 'John';"}
        ]
        generated_sql = llm_instance.generate_sql_from_natural_language(
            natural_language_query=nl_query,
            db_schema=schema,
            examples=few_shot_examples
        )
        print(f"Natural Language: {nl_query}\nGenerated SQL: {generated_sql}")

    # Example 3: Get text embedding
    if hasattr(llm_instance, "get_text_embedding"):
        print("\n[Example 3: Get Text Embedding]")
        text_to_embed = "Customer service was excellent and resolved my issue quickly."
        embedding_vector = llm_instance.get_text_embedding(text_input=text_to_embed)
        print(f"Text: '{text_to_embed}'\nEmbedding (first 5 dims): {embedding_vector[:5]}...")

    # Example 4: Get model capabilities (if such a method exists)
    if hasattr(llm_instance, "get_model_capabilities"):
        print("\n[Example 4: Get Model Capabilities]")
        capabilities = llm_instance.get_model_capabilities()
        print(f"Model: {capabilities.get('model_name')}, Type: {capabilities.get('type')}, Max Context: {capabilities.get('max_context_length')}")

    # Example 5: Generate text with different parameters
    print("\n[Example 5: Generate Text with different parameters]")
    prompt2 = "Write a short poem about coding."
    generated_poem = llm_instance.generate_text(prompt=prompt2, max_tokens=60, temperature=0.8, stop_sequences=["\n\n"])
    print(f"Prompt: {prompt2}\nLLM Response:\n{generated_poem}")


    print("\n--- LLMmodel Module Example Complete ---")

if __name__ == "__main__":
    main()
